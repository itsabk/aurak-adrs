# ADRS ‚Äì Automated Dataset Recommendation System

> A research project by American University of Ras Al Khaimah, Computer Science Department

ADRS leverages Large Language Models (LLMs) via OpenRouter to help users discover relevant datasets on the Hugging Face Hub based on their natural language research intent.

It aims to demonstrate an alternative, potentially faster or more intuitive, approach compared to traditional keyword-based searching on the Hub.

## Features

- **Natural Language Input:** Describe the dataset you need in plain English.
- **LLM-Powered Keyword Generation:** Uses an LLM (configurable via OpenRouter) to extract relevant search keywords from your intent.
- **Hugging Face Hub Search:** Searches the Hugging Face Hub using the generated keywords.
- **LLM-Powered Evaluation & Ranking:** Concurrently evaluates datasets using the LLM based on dynamic criteria, standardizes metadata, assigns relevance scores, and ranks results.
- **LLM-Powered Reporting:** Generates a textual report synthesizing the discovery process, key findings, and data landscape analysis.
- **Integrated Visualizations:** Creates interactive charts (e.g., score distribution, domain counts, task types, licenses, size estimates, and relevance vs. popularity) to accompany the report.
- **Two Operating Modes:** Choose between "Auto" mode for fully automated workflow or "Assistive" mode for step-by-step confirmation.
- **Web Interface:** Simple UI built with Streamlit for easy interaction.
- **Configurable:** Set API keys, select the LLM model, adjust search limits, and edit LLM prompts via the UI.
- **Fully Editable Prompts:** Edit the prompts for Keyword Generation, Criteria Generation, Dataset Evaluation, and Report Generation directly in the UI sidebar (Advanced).

## Setup

1.  **Clone the repository (or ensure you have the project files).**

2.  **Create and activate a Python virtual environment:**

    ```bash
    python3 -m venv venv
    source venv/bin/activate # On Windows use `venv\Scripts\activate`
    ```

3.  **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure API Keys:**
    - Create a `.env` file in the project root (you can copy `.env.example`).
    - Add your OpenRouter API key:
      ```dotenv
      OPENROUTER_API_KEY=YOUR_OPENROUTER_API_KEY_HERE
      ```
    - Optionally, add your Hugging Face Hub token (increases rate limits):
      ```dotenv
      HUGGINGFACE_HUB_TOKEN=YOUR_HUGGINGFACE_HUB_TOKEN_HERE
      ```
    - Alternatively, you can enter the keys directly into the Streamlit UI sidebar when running the app.

## Usage

1.  **Run the Streamlit application:**

    ```bash
    streamlit run app.py
    ```

2.  **Open the URL** provided in your terminal (usually `http://localhost:8501`) in your web browser.

3.  **Configure (Sidebar):**

    - Ensure your OpenRouter API Key is entered (loaded from `.env` or pasted).
    - Optionally enter your Hugging Face Hub Token.
    - Optionally change the LLM Model identifier.
    - Select operating mode ("Auto" or "Assistive").
    - Optionally configure Search Limits (`Max Keywords`, `Fetch Limit`, `Final Limit`).
    - Optionally edit the prompts used for all LLM interactions via the "üìù Edit Prompts" expander in the sidebar (Advanced).

4.  **Enter Intent:** Type your research intent into the main text area (e.g., "Datasets for sentiment analysis of product reviews") or select from the provided examples.

5.  **Click "Discover Datasets".**

6.  **Workflow Steps:**

    **Auto Mode:**

    - The system automatically generates keywords, criteria, searches Hugging Face Hub, evaluates datasets, and displays results.
    - Report and visualizations are automatically generated.

    **Assistive Mode:**

    - **Step 1:** Review and edit generated keywords
    - **Step 2:** Review and edit evaluation criteria
    - **Step 3:** System searches Hugging Face Hub and evaluates datasets
    - **Step 4:** System displays results
    - **Step 5:** Optionally generate report and visualizations

7.  **View Results:** The app displays a table of evaluated datasets from Hugging Face Hub, sorted by the LLM's relevance score.

8.  **View Report:** The system generates:
    - Interactive visualizations (score distribution, domain counts, task types, license distribution, etc.)
    - A detailed textual report generated by the LLM, summarizing the process and findings, and referencing the visualizations.

## How it Works

1.  **Input:** The user provides a research intent.

2.  **Mode Selection:** The user selects between "Auto" or "Assistive" mode.

3.  **Keyword Generation:** The intent is sent to the configured LLM (LiteLLM/OpenRouter) using the keyword generation prompt to generate relevant comma-separated search keywords.

4.  **Keyword Refinement (Assistive Mode):** The user confirms or edits the keywords.

5.  **Criteria Generation:** The user intent is sent to the LLM again to generate a dynamic, task-specific list of evaluation criteria as a JSON list.

6.  **Criteria Refinement (Assistive Mode):** The user confirms or edits the evaluation criteria.

7.  **HF Search:** The keywords are used to search the Hugging Face Hub (`list_datasets` with `full=True`), respecting the configured search limits.

8.  **Aggregation:** Unique raw dataset metadata records are collected.

9.  **Concurrent LLM Evaluation:**

    - The user intent, dynamic criteria, and raw metadata for each dataset are sent concurrently to the LLM.
    - The LLM evaluates each dataset against the criteria and assigns a relevance score (0.0-1.0).
    - The LLM extracts/infers standardized fields and provides reasoning for the score.

10. **Sorting & Display:** Datasets are sorted by relevance score and displayed in a table.

11. **Report Generation:**
    - In Auto mode: Automatically generates visualizations and a report.
    - In Assistive mode: User can click "Generate Full Report & Visualizations" button.
    - The system generates interactive visualizations including:
      - Relevance score distribution
      - Domain counts
      - Task type counts
      - License distribution
      - Data size estimates
      - Relevance score vs. popularity (downloads) scatter plot
    - The LLM generates a comprehensive report analyzing the datasets and referencing the visualizations.

## System Components

The ADRS system consists of several key components:

1. **Web Interface (app.py):** Streamlit-based user interface for interaction and visualization.

2. **LLM Interaction (src/llm_interaction.py):** Handles communication with LLMs via LiteLLM/OpenRouter.

3. **Hugging Face Search (src/hf_search.py):** Manages dataset discovery on the Hugging Face Hub.

4. **Metadata Schema (src/metadata_schema.py):** Defines the structure for dataset metadata and evaluations.

5. **Prompt Templates (prompts/):**
   - `generate_keywords.txt`: Extracts search keywords from user intent
   - `generate_criteria.txt`: Creates evaluation criteria based on user intent
   - `evaluate_dataset.txt`: Evaluates datasets against criteria
   - `generate_report.txt`: Synthesizes findings into a comprehensive report
   - `standardize_metadata.txt`: Template for metadata standardization

## Attribution

ADRS is a research project developed by the Computer Science Department at American University of Ras Al Khaimah. This tool demonstrates the application of Large Language Models to improve dataset discovery workflows.

## Metadata Evaluation Format

After retrieving raw metadata from Hugging Face, the system uses an LLM to perform an integrated evaluation and standardization step based on the user's intent. The LLM analyzes the raw metadata and generates the following structure (defined in `src/metadata_schema.py` as `EvaluatedMetadata`). Fields are optional (`Optional[...]`) and will be included if the LLM can confidently extract, infer, or determine them.

```python
class EvaluatedMetadata(TypedDict, total=False):
    """
    Defines the structure after integrated LLM evaluation, including both
    extracted/inferred fields and evaluation metrics based on user intent.
    Fields added *after* LLM call (like id, url, downloads, likes) are also included here.
    """
    # --- Added by the system after LLM evaluation ---
    id: Optional[str]                 # Original Hugging Face Dataset ID
    url: Optional[str]                # Direct URL to the Hugging Face dataset page
    downloads: Optional[int]          # Raw downloads count from HF
    likes: Optional[int]              # Raw likes count from HF

    # --- Extracted/Inferred/Evaluated by LLM ---
    clear_summary: Optional[str]        # Concise summary of the dataset's purpose and content.
    domain: Optional[str]               # Primary domain (e.g., Healthcare, NLP).
    task_type: Optional[str]            # Primary ML task suitability (e.g., Classification).
    data_size_estimate: Optional[str]   # Estimated size (e.g., "Medium (1k-100k rows)").
    key_features_columns: Optional[List[str]] # List of key features, columns, or data fields.
    data_quality_hints: Optional[List[str]] # Hints about data quality, collection, limitations.
    potential_biases_mentioned: Optional[List[str]] # Potential biases explicitly mentioned.
    license_type: Optional[str]         # Dataset license (e.g., 'apache-2.0', 'mit').
    relevance_score: Optional[float]    # LLM-assigned score (0.0-1.0) based on intent.
    reasoning: Optional[str]            # LLM-generated explanation for the score.
```

This integrated format provides both standardized information and an intent-based relevance assessment in a single step, with results sorted by the LLM's score.

## Future Enhancements

- Direct download of selected datasets
- More in-depth dataset analysis using LLMs
- Compatibility assessment for merging multiple datasets
- Custom filtering and sorting options
- Support for additional dataset repositories beyond Hugging Face
- Export options for reports and visualizations
